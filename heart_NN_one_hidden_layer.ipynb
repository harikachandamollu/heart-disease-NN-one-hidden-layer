{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary classification with one hidden layer(Neural Network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data description and analysis is carried out in heart_exploratory_data_analysis.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing necessary libraries to import the data and processing the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading dataset from csv file into numpy array. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[nan, nan, nan, ..., nan, nan, nan],\n",
       "       [63.,  1.,  3., ...,  0.,  1.,  1.],\n",
       "       [37.,  1.,  2., ...,  0.,  2.,  1.],\n",
       "       ...,\n",
       "       [68.,  1.,  0., ...,  2.,  3.,  0.],\n",
       "       [57.,  1.,  0., ...,  1.,  3.,  0.],\n",
       "       [57.,  0.,  1., ...,  1.,  2.,  0.]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.genfromtxt('heart.csv',delimiter=',')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Slicing the data and storing the input variables in X and target variable in Y, in the form of matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type of X: <class 'numpy.ndarray'>\n",
      "type of Y: <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "X=np.array((data[1:,:-1]))\n",
    "Y=np.array((data[1:,-1]))\n",
    "print('type of X:',type(X))\n",
    "print('type of Y:',type(Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make sure the slicing and dimensions(rows, columns) are correct, and training set size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training examples: 303\n",
      "Shape of input variables(X-matrix/numpy array): (303, 13)\n",
      "Shape of target variable(Y-vector/numpy array): (303,)\n"
     ]
    }
   ],
   "source": [
    "print('number of training examples:',X.shape[0])\n",
    "print('Shape of input variables(X-matrix/numpy array):',X.shape)\n",
    "print('Shape of target variable(Y-vector/numpy array):',Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To avoid further errors, converting dimensions of Y vector to one-column matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(303, 1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = np.reshape(Y,(Y.shape[0],1))\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting the data into train and test data where 20% of the data is the test data. Using shape, to confirm the split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X(train_set): (242, 13)\n",
      "Shape of X(test_set): (61, 13)\n",
      "Shape of Y(train_set): (242, 1)\n",
      "Shape of Y(test_set): (61, 1)\n"
     ]
    }
   ],
   "source": [
    "train_X, test_X, train_Y, test_Y = train_test_split(X,Y,test_size=0.2,random_state=0)\n",
    "print(\"Shape of X(train_set):\",train_X.shape)\n",
    "print(\"Shape of X(test_set):\",test_X.shape)\n",
    "print(\"Shape of Y(train_set):\",train_Y.shape)\n",
    "print(\"Shape of Y(test_set):\",test_Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transpose of the data for easy calculations further"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X(train_set): (13, 242)\n",
      "Shape of X(test_set): (13, 61)\n",
      "Shape of Y(train_set): (1, 242)\n",
      "Shape of Y(test_set): (1, 61)\n"
     ]
    }
   ],
   "source": [
    "train_X=train_X.T\n",
    "test_X=test_X.T\n",
    "train_Y=train_Y.T\n",
    "test_Y=test_Y.T\n",
    "print(\"Shape of X(train_set):\",train_X.shape)\n",
    "print(\"Shape of X(test_set):\",test_X.shape)\n",
    "print(\"Shape of Y(train_set):\",train_Y.shape)\n",
    "print(\"Shape of Y(test_set):\",test_Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network model with single hidden layer. \n",
    "#### General Methodology:\n",
    "1. Define the neural network structure(#input_units, #hidden_units, #output_units)\n",
    "2. Initialize the model's parameters\n",
    "3. Loop:\n",
    "    1. Implement forward propagation\n",
    "    2. Compute loss\n",
    "    3. Implement backward propagation to get the gradients\n",
    "    4. Update parameters (gradient descent)\n",
    "4. Merge 1,2,3 steps into one function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Define the neural network structure(input)\n",
    "- Arguments:\n",
    "    - X -- input dataset of shape (input size, number of examples)\n",
    "    - Y -- labels of shape (output size, number of examples)  \n",
    "- Returns:\n",
    "    - n_x -- the size of the input layer\n",
    "    - n_h -- the size of the hidden layer (=4)\n",
    "    - n_y -- the size of the output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_sizes(X, Y):\n",
    "    \n",
    "    n_x = X.shape[0] #size of input layer\n",
    "    n_h = 4\n",
    "    n_y = Y.shape[0] #size of output layer\n",
    "    \n",
    "    return (n_x, n_h, n_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Initialize the model's parameters\n",
    "- Argument:\n",
    "    - n_x -- size of the input layer\n",
    "    - n_h -- size of the hidden layer\n",
    "    - n_y -- size of the output layer\n",
    "- Returns: params -- python dictionary containing your parameters:\n",
    "    - W1 -- weight matrix of shape (n_h, n_x)\n",
    "    - b1 -- bias vector of shape (n_h, 1)\n",
    "    - W2 -- weight matrix of shape (n_y, n_h)\n",
    "    - b2 -- bias vector of shape (n_y, 1)\n",
    "- Initializing weights matrices with random values\n",
    "- Initializing vias vectors with zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    \n",
    "    W1 = np.random.randn(n_h, n_x)*0.01\n",
    "    b1 = np.zeros((n_h, 1))\n",
    "    W2 = np.random.randn(n_y, n_h)*0.01\n",
    "    b2 = np.zeros((n_y,1))\n",
    "    \n",
    "    assert (W1.shape == (n_h, n_x))\n",
    "    assert (b1.shape == (n_h, 1))\n",
    "    assert (W2.shape == (n_y, n_h))\n",
    "    assert (b2.shape == (n_y, 1))\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Loop:\n",
    "#### 3.A. Implement forward propagation\n",
    "- Argument:\n",
    "    - X -- input data of size (n_x, m)\n",
    "    - parameters -- python dictionary containing your parameters (output of initialization function)\n",
    "- Returns:\n",
    "    - A2 -- The sigmoid output of the second activation\n",
    "    - cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters):\n",
    "\n",
    "    # Retrieve each parameter from the dictionary \"parameters\"\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    \n",
    "    # Implement Forward Propagation to calculate A2 (probabilities)\n",
    "    Z1 = np.dot(W1,X)+b1\n",
    "    A1 = np.tanh(Z1)\n",
    "    Z2 = np.dot(W2,A1)+b2\n",
    "    A2 = sigmoid(Z2)\n",
    "    \n",
    "    assert(A2.shape == (1, X.shape[1]))\n",
    "    \n",
    "    cache = {\"Z1\": Z1,\n",
    "             \"A1\": A1,\n",
    "             \"Z2\": Z2,\n",
    "             \"A2\": A2}\n",
    "    \n",
    "    return A2, cache\n",
    "\n",
    "#Sigmoid function\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid of x\n",
    "\n",
    "    Arguments:\n",
    "    x -- A scalar or numpy array of any size.\n",
    "\n",
    "    Return:\n",
    "    s -- sigmoid(x)\n",
    "    \"\"\"\n",
    "    s = 1/(1+np.exp(-x))\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.B. Compute cost\n",
    "\n",
    "- Computes the cross-entropy cost\n",
    "- Arguments:\n",
    "    - A2 -- The sigmoid output of the second activation, of shape (1, number of examples)\n",
    "    - Y -- \"true\" labels vector of shape (1, number of examples)\n",
    "    - parameters -- python dictionary containing your parameters W1, b1, W2 and b2\n",
    "- Returns:\n",
    "    - cost -- cross-entropy cost given equation (13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(A2, Y, parameters):\n",
    "    \n",
    "    m = Y.shape[1] # number of example\n",
    "\n",
    "    # Compute the cross-entropy cost\n",
    "    logprobs = np.multiply(np.log(A2),Y)+np.multiply(np.log(1-A2),1-Y)\n",
    "    cost = - np.sum(logprobs)\n",
    "    \n",
    "    cost = float(np.squeeze(cost))  # makes sure cost is the dimension we expect. \n",
    "                                    # E.g., turns [[17]] into 17 \n",
    "    assert(isinstance(cost, float))\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.C. Implement backward propagation to get the gradients\n",
    "- Arguments:\n",
    "    - parameters -- python dictionary containing our parameters \n",
    "    - cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\".\n",
    "    - X -- input data of shape (2, number of examples)\n",
    "    - Y -- \"true\" labels vector of shape (1, number of examples)\n",
    "- Returns:\n",
    "    - grads -- python dictionary containing your gradients with respect to different parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(parameters, cache, X, Y):\n",
    "\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    # First, retrieve W1 and W2 from the dictionary \"parameters\".\n",
    "    W1 = parameters[\"W1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "        \n",
    "    # Retrieve also A1 and A2 from dictionary \"cache\".\n",
    "    A1 = cache[\"A1\"]\n",
    "    A2 = cache[\"A2\"]\n",
    "    \n",
    "    # Backward propagation: calculate dW1, db1, dW2, db2. \n",
    "    dZ2 = A2-Y\n",
    "    dW2 = (1/m)*np.dot(dZ2,A1.T)\n",
    "    db2 = (1/m)*np.sum(dZ2, axis=1, keepdims=True)\n",
    "    dZ1 = np.dot(W2.T,dZ2)*(1-np.power(A1,2))\n",
    "    dW1 = (1/m)*np.dot(dZ1,X.T)\n",
    "    db1 = (1/m)*np.sum(dZ1, axis=1, keepdims=True)\n",
    "    \n",
    "    grads = {\"dW1\": dW1,\n",
    "             \"db1\": db1,\n",
    "             \"dW2\": dW2,\n",
    "             \"db2\": db2}\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update Parameters(Gradient Descent)\n",
    "- Arguments:\n",
    "    - parameters -- python dictionary containing your parameters \n",
    "    - grads -- python dictionary containing your gradients \n",
    "- Returns:\n",
    "    - parameters -- python dictionary containing your updated parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate = 1.2):\n",
    "    \n",
    "    # Retrieve each parameter from the dictionary \"parameters\"\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    \n",
    "    # Retrieve each gradient from the dictionary \"grads\"\n",
    "    dW1 = grads[\"dW1\"]\n",
    "    db1 = grads[\"db1\"]\n",
    "    dW2 = grads[\"dW2\"]\n",
    "    db2 = grads[\"db2\"]\n",
    "    \n",
    "    # Update rule for each parameter\n",
    "    W1 = W1-(learning_rate*dW1)\n",
    "    b1 = b1-(learning_rate*db1)\n",
    "    W2 = W2-(learning_rate*dW2)\n",
    "    b2 = b2-(learning_rate*db2)\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Merge steps 1,2,3 into one functions \n",
    "- Arguments:\n",
    "    - X -- dataset of shape (13, number of examples)\n",
    "    - Y -- labels of shape (1, number of examples)\n",
    "    - n_h -- size of the hidden layer\n",
    "    - num_iterations -- Number of iterations in gradient descent loop\n",
    "    - print_cost -- if True, print the cost every 1000 iterations\n",
    "- Returns:\n",
    "    - parameters -- parameters learnt by the model. They can then be used to predict.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_model(X, Y, n_h, num_iterations = 10000, print_cost=False):\n",
    "    \n",
    "    np.random.seed(3)\n",
    "    n_x = layer_sizes(X, Y)[0] #1\n",
    "    n_y = layer_sizes(X, Y)[2] #1\n",
    "    \n",
    "    # Initialize parameters #2\n",
    "    parameters = initialize_parameters(n_x, n_h, n_y)\n",
    "    \n",
    "    # Loop (gradient descent) #3\n",
    "    for i in range(0, num_iterations):\n",
    "         \n",
    "        # Forward propagation. Inputs: \"X, parameters\". Outputs: \"A2, cache\". #3.A\n",
    "        A2, cache = forward_propagation(X, parameters)\n",
    "        \n",
    "        # Cost function. Inputs: \"A2, Y, parameters\". Outputs: \"cost\". #3.B\n",
    "        cost = compute_cost(A2, Y, parameters)\n",
    " \n",
    "        # Backpropagation. Inputs: \"parameters, cache, X, Y\". Outputs: \"grads\". #3.C\n",
    "        grads = backward_propagation(parameters, cache, X, Y)\n",
    " \n",
    "        # Gradient descent parameter update. Inputs: \"parameters, grads\". Outputs: \"parameters\". #3.D\n",
    "        parameters = update_parameters(parameters, grads)\n",
    "        \n",
    "        # Print the cost every 1000 iterations\n",
    "        if print_cost and i % 1000 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predicting the target \n",
    "- Using the learned parameters, predicts a class for each example in X\n",
    "- Arguments:\n",
    "    - parameters -- python dictionary containing your parameters \n",
    "    - X -- input data of size (n_x, m)\n",
    "- Returns\n",
    "    - predictions -- vector of predictions of our model (red: 0 / blue: 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(parameters, X):\n",
    "    \n",
    "    # Computes probabilities using forward propagation, and \n",
    "    # classifies to 0/1 using 0.5 as the threshold.\n",
    "    A2, cache = forward_propagation(X, parameters)\n",
    "    predictions = np.round(A2)\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the model with X and Y train data sets using nn_model function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 167.835480\n",
      "Cost after iteration 1000: 166.914228\n",
      "Cost after iteration 2000: 166.914228\n",
      "Cost after iteration 3000: 166.914228\n",
      "Cost after iteration 4000: 166.914228\n",
      "Cost after iteration 5000: 166.914228\n",
      "Cost after iteration 6000: 166.914228\n",
      "Cost after iteration 7000: 166.914228\n",
      "Cost after iteration 8000: 166.914228\n",
      "Cost after iteration 9000: 166.914228\n"
     ]
    }
   ],
   "source": [
    "# Build a model with a n_h-dimensional hidden layer\n",
    "parameters = nn_model(train_X, train_Y, n_h = 4, num_iterations = 10000, print_cost=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the above obtained parameters to predict the test set and accuracy is calculated based on train and test set of Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:55.737705%\n"
     ]
    }
   ],
   "source": [
    "predictions = predict(parameters, test_X)\n",
    "print ('Accuracy:%f' %float((np.dot(test_Y,predictions.T) + np.dot(1-test_Y,1-predictions.T))/float(test_Y.size)*100) + '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tuning the hidden layer units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for 1 hidden units: 55.73770491803278 %\n",
      "Accuracy for 2 hidden units: 55.73770491803278 %\n",
      "Accuracy for 3 hidden units: 55.73770491803278 %\n",
      "Accuracy for 4 hidden units: 55.73770491803278 %\n",
      "Accuracy for 5 hidden units: 55.73770491803278 %\n",
      "Accuracy for 6 hidden units: 44.26229508196721 %\n",
      "Accuracy for 7 hidden units: 44.26229508196721 %\n",
      "Accuracy for 8 hidden units: 44.26229508196721 %\n",
      "Accuracy for 9 hidden units: 44.26229508196721 %\n",
      "Accuracy for 10 hidden units: 44.26229508196721 %\n",
      "Accuracy for 20 hidden units: 44.26229508196721 %\n",
      "Accuracy for 30 hidden units: 44.26229508196721 %\n",
      "Accuracy for 40 hidden units: 44.26229508196721 %\n",
      "Accuracy for 50 hidden units: 55.73770491803278 %\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "hidden_layer_units = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 20, 30, 40, 50]\n",
    "for i, n_h in enumerate(hidden_layer_units):\n",
    "    parameters = nn_model(train_X, train_Y, n_h, num_iterations = 5000)\n",
    "    predictions = predict(parameters, test_X)\n",
    "    accuracy = float((np.dot(test_Y,predictions.T) + np.dot(1-test_Y,1-predictions.T))/float(test_Y.size)*100)\n",
    "    print (\"Accuracy for {} hidden units: {} %\".format(n_h, accuracy))\n",
    "    plt.plot([n_h],[accuracy],'rx')\n",
    "    plt.xlabel('Number of hidden units')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Accuracy changes with respect to hidden units')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From the above chart we can infer that only high variation in the number of hidden units results in change in accuracy. Very high number of hidden units also results in overfitting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
